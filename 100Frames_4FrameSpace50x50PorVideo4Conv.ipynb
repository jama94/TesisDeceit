{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "100Frames_4FrameSpace50x50PorVideo4Conv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBoXseK-M3qm",
        "colab_type": "code",
        "outputId": "5ed6a6f2-5087-43e6-dc39-49456b6fac00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticxxate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131183 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.13-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.13-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5MmI0KYMs9",
        "colab_type": "code",
        "outputId": "149d5759-fa58-4e0a-df18-207204c68de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import *\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTTwVBQyM3s1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZuhDZe1te-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tensorFlow para entrenar la red\n",
        "import tensorflow as tf\n",
        "\n",
        "# numpy :D\n",
        "import numpy as np\n",
        "\n",
        "# Imagenes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "#libreria números pseudo random\n",
        "import random\n",
        "random.seed(a=None)\n",
        "\n",
        "\n",
        "# Graficado\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcmHmE4ePPXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'drive/Colab Notebooks/'\n",
        "save_file = path + 'checkpoint/train_model.ckpt'\n",
        "data_file = path + 'Data'\n",
        "sizeimage = 50\n",
        "Frames = 100\n",
        "filtros = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rB_u_FtM3uz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTrainR=np.load(data_file + str(Frames) + \"/TrainR\" + str(Frames) + \".npy\")\n",
        "#dataTrainG=np.load(data_file + str(Frames) + \"/TrainG\" + str(Frames) + \".npy\")\n",
        "#dataTrainB=np.load(data_file + str(Frames) + \"/TrainB\" + str(Frames) + \".npy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1QOa_5A-g5g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTrainGS=np.load(data_file + str(Frames) + \"Cara/TrainGS\" + str(Frames) + \".npy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYjBVc6DDh3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTrainSX=np.load(data_file + str(Frames) + \"Cara/TrainSX\" + str(Frames) + \".npy\")\n",
        "dataTrainSY=np.load(data_file + str(Frames) + \"Cara/TrainSY\" + str(Frames) + \".npy\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lTwt4X5Dh6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTrainLAP=np.load(data_file + str(Frames) + \"/TrainLAP\" + str(Frames) + \".npy\")\n",
        "dataTrainKIR=np.load(data_file + str(Frames) + \"Cara/TrainKIR\" + str(Frames) + \".npy\")#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a8gP3m5Dh9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTrainOY=np.load(data_file + str(Frames) + \"/TrainOY\" + str(Frames) + \".npy\")\n",
        "#dataTrainOX=np.load(data_file + str(Frames) + \"/TrainOX\" + str(Frames) + \".npy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8GWlFE_-hLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTrainY=np.load(data_file + str(Frames) + \"Cara/TrainY\" + str(Frames) + \".npy\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lz46vAZXAsBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#len(dataTrainKIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD044jQkK931",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(dataTrainKIR.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8JOfkETAvAQ",
        "colab_type": "code",
        "outputId": "83ff3229-d955-4e2a-c12f-d453fcb66812",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(dataTrainY)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "643"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw-uT2BjE6pl",
        "colab_type": "text"
      },
      "source": [
        "Este de abajo cambia dependiendo de que filtros este utilizando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjgBnZYcM35i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTrainGS,dataTrainY = shuffle(dataTrainGS,dataTrainY)\n",
        "\n",
        "#dataTrainR,dataTrainG,dataTrainB,dataTrainY = shuffle(dataTrainR,dataTrainG,dataTrainB,dataTrainY)\n",
        "\n",
        "#dataTrainSX,dataTrainSY,dataTrainY = shuffle(dataTrainSX,dataTrainSY,dataTrainY)\n",
        "\n",
        "#dataTrainOY,dataTrainOX,dataTrainY = shuffle(dataTrainOY,dataTrainOX,dataTrainY)\n",
        "\n",
        "#dataTrainGS,dataTrainOY,dataTrainOX,dataTrainY = shuffle(dataTrainGS,dataTrainOY,dataTrainOX,dataTrainY)\n",
        "\n",
        "#dataTrainGS,dataTrainKIR,dataTrainY = shuffle(dataTrainGS,dataTrainKIR,dataTrainY)\n",
        "\n",
        "#dataTrainKIR,dataTrainY = shuffle(dataTrainKIR,dataTrainY)\n",
        "\n",
        "#dataTrainGS,dataTrainSY,dataTrainSX,dataTrainY = shuffle(dataTrainGS,dataTrainSY,dataTrainSX,dataTrainY)\n",
        "dataTrainGS,dataTrainSY,dataTrainSX,dataTrainKIR,dataTrainY = shuffle(dataTrainGS,dataTrainSY,dataTrainSX,dataTrainKIR,dataTrainY)\n",
        "#dataTrainKIR,dataTrainSY,dataTrainSX,dataTrainY = shuffle(dataTrainKIR,dataTrainSY,dataTrainSX,dataTrainY)\n",
        "\n",
        "#dataTrainKIR,dataTrainOY,dataTrainOX,dataTrainY = shuffle(dataTrainKIR,dataTrainOY,dataTrainOX,dataTrainY)\n",
        "\n",
        "#dataTrainGS,dataTrainLAP,dataTrainKIR,dataTrainY = shuffle(dataTrainGS,dataTrainLAP,dataTrainKIR,dataTrainY)\n",
        "#dataTrainLAP,dataTrainY = shuffle(dataTrainLAP,dataTrainY)\n",
        "\n",
        "#dataTrainKIR,dataTrainOY,dataTrainOX,dataTrainY = shuffle(dataTrainKIR,dataTrainOY,dataTrainOX,dataTrainY)\n",
        "#dataTrainGS,dataTrainSY,dataTrainSY,dataTrainLAP,dataTrainKIR,dataTrainY = shuffle(dataTrainGS,dataTrainSY,dataTrainSY,dataTrainLAP,dataTrainKIR,dataTrainY)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbpWPHsAy-av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTrainGS = dataTrainGS[int(len(dataTrainGS)/10):]\n",
        "#dataTrainGS2 = dataTrainGS[:int(len(dataTrainGS)/10)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/10):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/10)]\n",
        "\n",
        "\n",
        "dataTrainGS = dataTrainGS[int(len(dataTrainGS)/20):]\n",
        "dataTrainGS2 = dataTrainGS[:int(len(dataTrainGS)/20)]\n",
        "dataTrainSY = dataTrainSY[int(len(dataTrainSY)/20):]\n",
        "dataTrainSY2 = dataTrainSY[:int(len(dataTrainSY)/20)]\n",
        "dataTrainSX = dataTrainSX[int(len(dataTrainSX)/20):]\n",
        "dataTrainSX2 = dataTrainSX[:int(len(dataTrainSX)/20)]\n",
        "dataTrainY = dataTrainY[int(len(dataTrainY)/20):]\n",
        "dataTrainY2 = dataTrainY[:int(len(dataTrainY)/20)]\n",
        "\n",
        "#dataTrainGS = dataTrainGS[int(len(dataTrainGS)/10):]\n",
        "#dataTrainGS2 = dataTrainGS[:int(len(dataTrainGS)/10)]\n",
        "#dataTrainOY = dataTrainOY[int(len(dataTrainOY)/10):]\n",
        "#dataTrainOY2 = dataTrainOY[:int(len(dataTrainOY)/10)]\n",
        "#dataTrainOX = dataTrainOX[int(len(dataTrainOX)/10):]\n",
        "#dataTrainOX2 = dataTrainOX[:int(len(dataTrainOX)/10)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/10):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/10)]\n",
        "\n",
        "\n",
        "#dataTrainLAP = dataTrainLAP[int(len(dataTrainLAP)/10):]\n",
        "#dataTrainLAP2 = dataTrainLAP[:int(len(dataTrainLAP)/10)]\n",
        "dataTrainKIR = dataTrainKIR[int(len(dataTrainKIR)/20):]\n",
        "dataTrainKIR2 = dataTrainKIR[:int(len(dataTrainKIR)/20)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/10):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/10)\n",
        "\n",
        "#dataTrainKIR = dataTrainKIR[int(len(dataTrainKIR)/20):]\n",
        "#dataTrainKIR2 = dataTrainKIR[:int(len(dataTrainKIR)/20)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/20):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/20)]\n",
        "\n",
        "#dataTrainGS = dataTrainGS[int(len(dataTrainGS)/10):]\n",
        "#dataTrainGS2 = dataTrainGS[:int(len(dataTrainGS)/10)]\n",
        "#dataTrainLAP = dataTrainLAP[int(len(dataTrainLAP)/10):]\n",
        "#dataTrainLAP2 = dataTrainLAP[:int(len(dataTrainLAP)/10)]\n",
        "#dataTrainKIR = dataTrainKIR[int(len(dataTrainKIR)/10):]\n",
        "#dataTrainKIR2 = dataTrainKIR[:int(len(dataTrainKIR)/10)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/10):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/10)]\n",
        "\n",
        "#dataTrainGS = dataTrainGS[int(len(dataTrainGS)/10):]\n",
        "#dataTrainGS2 = dataTrainGS[:int(len(dataTrainGS)/10)]\n",
        "#dataTrainOY = dataTrainOY[int(len(dataTrainOY)/10):]\n",
        "#dataTrainOY2 = dataTrainOY[:int(len(dataTrainOY)/10)]\n",
        "#dataTrainOX = dataTrainOX[int(len(dataTrainOX)/10):]\n",
        "#dataTrainOX2 = dataTrainOX[:int(len(dataTrainOX)/10)]\n",
        "#dataTrainY = dataTrainY[int(len(dataTrainY)/10):]\n",
        "#dataTrainY2 = dataTrainY[:int(len(dataTrainY)/10)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3blCDByOBYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"Data1 \",len(dataTrainR),\"Data2 \",len(dataTrainG),\"Data3 \",len(dataTrainB),\"  Datay  \",len(dataTrainY))\n",
        "\n",
        "#print(\"Data1 \",len(dataTrainGS),\"Data2 \",len(dataTrainSX),\"Data3 \",len(dataTrainSY),\"  Datay  \",len(dataTrainY))\n",
        "#print(\"Data1 \",len(dataTrainGS2),\"Data2 \",len(dataTrainSX2),\"Data3 \",len(dataTrainSY2),\"  Datay  \",len(dataTrainY2))\n",
        "\n",
        "#print(\"Data1 \",len(dataTrainGS),\"Data2 \",len(dataTrainLAP),\"Data3 \",len(dataTrainKIR),\"  Datay  \",len(dataTrainY))\n",
        "#print(\"Data1 \",len(dataTrainGS2),\"Data2 \",len(dataTrainLAP2),\"Data3 \",len(dataTrainKIR2),\"  Datay  \",len(dataTrainY2))\n",
        "\n",
        "\n",
        "#print(\"Data1 \",len(dataTrainKIR),\"Data2 \",len(dataTrainOY),\"Data3 \",len(dataTrainOX),\"  Datay  \",len(dataTrainY))\n",
        "\n",
        "#print(\"Data1 \",len(dataTrainR),\"Data2 \",len(dataTrainG),\"Data3 \",len(dataTrainB),\n",
        "#      \"Data4 \",len(dataTrainS),\"Data5 \",len(dataTrainSY),\"  Datay  \",len(dataTrainY))\n",
        "\n",
        "#print(\"Data1 \",len(dataTrainGS),\"Data2 \",len(dataTrainSX),\"Data3 \",len(dataTrainSY),\n",
        "#      \"Data4 \",len(dataTrainLAP),\"Data5 \",len(dataTrainKIR),\"  Datay  \",len(dataTrainY))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY7urBIyte-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for x in range(6):\n",
        "#    print(dataTrainY[x])\n",
        "#    plt.imshow(dataTrainGS[x][0], interpolation='none', cmap='gray')\n",
        "#    plt.xticks([]), plt.yticks([])\n",
        "#    plt.show()\n",
        "#    plt.imshow(dataTrainLAP[x][0], interpolation='none', cmap='gray')\n",
        "#    plt.xticks([]), plt.yticks([])\n",
        "#    plt.show()\n",
        "#    plt.imshow(dataTrainKIR[x][0], interpolation='none', cmap='gray')\n",
        "#    plt.xticks([]), plt.yticks([])\n",
        "#    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuXzwiksSIDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTestR=np.load(data_file + str(Frames) + \"/TestR\" + str(Frames) + \".npy\")\n",
        "#dataTestG=np.load(data_file + str(Frames) + \"/TestG\" + str(Frames) + \".npy\")\n",
        "#dataTestB=np.load(data_file + str(Frames) + \"/TestB\" + str(Frames) + \".npy\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4T16--GSIVT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTestGS=np.load(data_file + str(Frames) + \"Cara/TestGS\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestTruthGS2=np.load(data_file + str(Frames) + \"Cara/ValidationTruthGS\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestDeceitGS2=np.load(data_file + str(Frames) + \"Cara/ValidationDeceitGS\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7snG-5zSSIYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTestSX=np.load(data_file + str(Frames) + \"Cara/TestSX\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestSY=np.load(data_file + str(Frames) + \"Cara/TestSY\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "\n",
        "dataTestTruthSX2=np.load(data_file + str(Frames) + \"Cara/ValidationTruthSX\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestTruthSY2=np.load(data_file + str(Frames) + \"Cara/ValidationTruthSY\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "\n",
        "dataTestDeceitSX2=np.load(data_file + str(Frames) + \"Cara/ValidationDeceitSX\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestDeceitSY2=np.load(data_file + str(Frames) + \"Cara/ValidationDeceitSY\" + str(Frames) + \".npy\",allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClSi7lBeXgL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM-MGCQvWetQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTestLAP=np.load(data_file + str(Frames) + \"/TestLAP\" + str(Frames) + \".npy\")\n",
        "dataTestKIR=np.load(data_file + str(Frames) + \"Cara/TestKIR\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "\n",
        "#dataTestTruthLAP2=np.load(data_file + str(Frames) + \"/ValidationTruthLAP\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestTruthKIR2=np.load(data_file + str(Frames) + \"Cara/ValidationTruthKIR\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "\n",
        "#dataTestDeceitLAP2=np.load(data_file + str(Frames) + \"/ValidationDeceitLAP\" + str(Frames) + \".npy\")\n",
        "dataTestDeceitKIR2=np.load(data_file + str(Frames) + \"Cara/ValidationDeceitKIR\" + str(Frames) + \".npy\",allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXUQSvEzSIbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#dataTestOY=np.load(data_file + str(Frames) + \"/TestOY\" + str(Frames) + \".npy\")\n",
        "#dataTestOX=np.load(data_file + str(Frames) + \"/TestOX\" + str(Frames) + \".npy\")\n",
        "\n",
        "#dataTestOY2=np.load(data_file + str(Frames) + \"/ValidationOY\" + str(Frames) + \".npy\")\n",
        "#dataTestOX2=np.load(data_file + str(Frames) + \"/ValidationOX\" + str(Frames) + \".npy\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVM-ukreGwa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataTestY=np.load(data_file + str(Frames) + \"Cara/TestY\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestTruthY2=np.load(data_file + str(Frames) + \"Cara/ValidationTruthY\" + str(Frames) + \".npy\",allow_pickle=True)\n",
        "dataTestDeceitY2=np.load(data_file + str(Frames) + \"Cara/ValidationDeceitY\" + str(Frames) + \".npy\",allow_pickle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7JAu6SkGk7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(\"Data1 \",len(dataTestR),\"Data2 \",len(dataTestG),\"Data3 \",len(dataTestB),\"  Datay  \",len(dataTestY))\n",
        "\n",
        "#print(\"Data1 \",len(dataTestGS),\"Data2 \",len(dataTestSX),\"Data3 \",len(dataTestSY),\"Datay \",len(dataTestY))\n",
        "\n",
        "#print(\"Data1 \",len(dataTestGS),\"Data2 \",len(dataTestLAP),\"Data3 \",len(dataTestKIR),\"Datay \",len(dataTestY))\n",
        "#print(\"Data1 \",len(dataTestKIR),\"Data2 \",len(dataTestOY),\"Data3 \",len(dataTestOX),\"  Datay  \",len(dataTestY))\n",
        "#print(\"Data1 \",len(dataTestGS),\"Data2 \",len(dataTestSX),\"Data3 \",len(dataTestSY),\"  Data4  \",len(dataTestLAP),\n",
        "#      \"Data5 \",len(dataTestKIR),\"Datay \",len(dataTestY))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCRcgC6_fw-0",
        "colab_type": "code",
        "outputId": "0844a076-9ce5-49d3-f9ba-2b014c3caaac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#print(len(dataTestY))\n",
        "#print(dataTrainY)\n",
        "#print(dataTrainY2)\n",
        "#print(dataTestDeceitY2)\n",
        "\n",
        "print(len(dataTestDeceitY2[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4B7n6F0te-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.placeholder('float')\n",
        "y = tf.placeholder('float')\n",
        "n_classes = 2\n",
        "batch_size = 10\n",
        "keep_rate = 0.5\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8oxGJEKte-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3d(x, W):\n",
        "    return tf.nn.conv3d(x, W, strides=[1,1,1,1,1], padding='VALID')\n",
        "\n",
        "def maxpool3d(x):\n",
        "    #                        size of window         movement of window as you slide about\n",
        "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Rfz3BVOvDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Función para finalizar la sesión.\n",
        "def reset_graph():\n",
        "    if 'sess' in globals() and sess:\n",
        "        sess.close()\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvcv0_d9te-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convolutional_neural_network(\n",
        "    keep_rate = 1,\n",
        "    learn_rate= 1e-5,#antes erta 1618e-9\n",
        "    n_classes=2,\n",
        "    batch_size = 200\n",
        "    ):\n",
        "    reset_graph()\n",
        "    \n",
        "\n",
        "    maxpool1 = [1,2,2,2,1]\n",
        "    maxpool2 = [1,2,2,2,1]\n",
        "    maxpool3 = [1,2,3,3,1]\n",
        "    maxpool4 = [1,2,3,3,1]\n",
        "    c1 = 6\n",
        "    c2 = 16\n",
        "    c3 = 32\n",
        "    c4 = 128\n",
        "    #c5 = 512\n",
        "    #c6 = 1024\n",
        "    #cf = filtros*c4\n",
        "    cf = 1*c4\n",
        "    cf1 = 128\n",
        "    cf2 = 128\n",
        "    cf3 = 128\n",
        "    cf4 = 128\n",
        "    cf5 = 128\n",
        "    cf6 = 128\n",
        "    cf7 = 128\n",
        "    cf8 = 64\n",
        "    cf9 = 64\n",
        "    \n",
        "    xr   = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    xg   = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    xb   = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    \n",
        "    xgs   = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    \n",
        "    xsobx = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    xsoby = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "\n",
        "    xlap = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    xkir = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    \n",
        "    xoptx = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    xopty = tf.placeholder('float',[None,Frames,sizeimage,sizeimage])\n",
        "    \n",
        "    prob = tf.placeholder(tf.float32)\n",
        "\n",
        "    y = tf.placeholder('float')\n",
        "    #n_classes = 2\n",
        "    \n",
        "\n",
        "    #                # 3 imagenes x 7 x 7 kernel, 1 canal, 4 caracteristicas.\n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([9,3,3,1,c1])), #16\n",
        "               #       3imagenes x 8 x 8 kernel, 2 channels, 8 caracteristicas.\n",
        "               'W_conv2':tf.Variable(tf.random_normal([7,3,3,c1,c2])),\n",
        "               #       3imagenes x 8 x 8 kernel, 6 channels, 16 caracteristicas.\n",
        "               'W_conv3':tf.Variable(tf.random_normal([7,3,3,c2,c3])),\n",
        "               'W_conv4':tf.Variable(tf.random_normal([7,3,3,c3,c4])),#128\n",
        "               #'W_conv5':tf.Variable(tf.random_normal([2,3,3,c4,c5])),#128\n",
        "               #'W_conv6':tf.Variable(tf.random_normal([2,3,3,c5,c6])),#128\n",
        "               'W_fc':tf.Variable(tf.random_normal([cf,cf])),#384\n",
        "               'W_fc1':tf.Variable(tf.random_normal([cf,cf1])),\n",
        "               'W_fc2':tf.Variable(tf.random_normal([cf1,cf2])),\n",
        "               'W_fc3':tf.Variable(tf.random_normal([cf2,cf3])),\n",
        "               'W_fc4':tf.Variable(tf.random_normal([cf3,cf4])),\n",
        "               'W_fc5':tf.Variable(tf.random_normal([cf4,cf5])),\n",
        "               'W_fc6':tf.Variable(tf.random_normal([cf5,cf6])),\n",
        "               'W_fc7':tf.Variable(tf.random_normal([cf6,cf7])),\n",
        "               'W_fc8':tf.Variable(tf.random_normal([cf7,cf8])),\n",
        "               'W_fc9':tf.Variable(tf.random_normal([cf8,cf9])),\n",
        "               'out':tf.Variable(tf.random_normal([cf9, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([c1])),\n",
        "              'b_conv2':tf.Variable(tf.random_normal([c2])),\n",
        "              'b_conv3':tf.Variable(tf.random_normal([c3])),\n",
        "              'b_conv4':tf.Variable(tf.random_normal([c4])),\n",
        "              #'b_conv5':tf.Variable(tf.random_normal([c5])),\n",
        "              #'b_conv6':tf.Variable(tf.random_normal([c6])),\n",
        "              'b_fc':tf.Variable(tf.random_normal([cf])),\n",
        "              'b_fc1':tf.Variable(tf.random_normal([cf1])),\n",
        "              'b_fc2':tf.Variable(tf.random_normal([cf2])),\n",
        "              'b_fc3':tf.Variable(tf.random_normal([cf3])),\n",
        "              'b_fc4':tf.Variable(tf.random_normal([cf4])),\n",
        "              'b_fc5':tf.Variable(tf.random_normal([cf5])),\n",
        "              'b_fc6':tf.Variable(tf.random_normal([cf6])),\n",
        "              'b_fc7':tf.Variable(tf.random_normal([cf7])),\n",
        "              'b_fc8':tf.Variable(tf.random_normal([cf8])),\n",
        "              'b_fc9':tf.Variable(tf.random_normal([cf9])),\n",
        "              'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "    #                             image z   image Y   image x\n",
        "    xr1   = tf.reshape(xr,   shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    xg1   = tf.reshape(xg,   shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    xb1   = tf.reshape(xb,   shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    \n",
        "    xgs1   = tf.reshape(xgs,   shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    \n",
        "    xsobx1 = tf.reshape(xsobx, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    xsoby1 = tf.reshape(xsoby, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    \n",
        "    xlap1 = tf.reshape(xlap, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    xkir1 = tf.reshape(xkir, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    \n",
        "    xoptx1 = tf.reshape(xoptx, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "    xopty1 = tf.reshape(xopty, shape=[-1,      Frames,      sizeimage,      sizeimage,      1])\n",
        "\n",
        "    print(\"filtro\")\n",
        "    #xgs1 = tf.keras.layers.BatchNormalization()(xgs1, training=True)\n",
        "    conv1gs = tf.nn.relu(conv3d(xgs1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1gs)\n",
        "    conv1gs = tf.nn.max_pool3d(conv1gs, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1gs)\n",
        "    conv2gs = tf.nn.relu(conv3d(conv1gs, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2gs)\n",
        "    conv2gs = tf.nn.max_pool3d(conv2gs, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2gs)\n",
        "    conv3gs = tf.nn.relu(conv3d(conv2gs, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3gs)\n",
        "    conv3gs = tf.nn.max_pool3d(conv3gs, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3gs)\n",
        "    conv4gs = tf.nn.relu(conv3d(conv3gs, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4gs)\n",
        "    \n",
        "    #xsobx1 = tf.keras.layers.BatchNormalization()(xsobx1, training=True)\n",
        "    print(\"filtro\")\n",
        "    conv1sobx = tf.nn.relu(conv3d(xsobx1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1sobx)\n",
        "    conv1sobx = tf.nn.max_pool3d(conv1sobx, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1sobx)\n",
        "    conv2sobx = tf.nn.relu(conv3d(conv1sobx, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2sobx)\n",
        "    conv2sobx = tf.nn.max_pool3d(conv2sobx, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2sobx)\n",
        "    conv3sobx = tf.nn.relu(conv3d(conv2sobx, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3sobx)\n",
        "    conv3sobx = tf.nn.max_pool3d(conv3sobx, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3sobx)\n",
        "    conv4sobx = tf.nn.relu(conv3d(conv3sobx, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4sobx)\n",
        "    #CNN3DSX = tf.keras.layers.BatchNormalization()(conv4sobx, training=True)\n",
        "    \n",
        "    #xsoby1 = tf.keras.layers.BatchNormalization()(xsoby1, training=True)\n",
        "    print(\"filtro\")\n",
        "    conv1soby = tf.nn.relu(conv3d(xsoby1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1soby)\n",
        "    conv1soby = tf.nn.max_pool3d(conv1soby, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1soby)\n",
        "    conv2soby = tf.nn.relu(conv3d(conv1soby, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2soby)\n",
        "    conv2soby = tf.nn.max_pool3d(conv2soby, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2soby)\n",
        "    conv3soby = tf.nn.relu(conv3d(conv2soby, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3soby)\n",
        "    conv3soby = tf.nn.max_pool3d(conv3soby, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3soby)\n",
        "    conv4soby = tf.nn.relu(conv3d(conv3soby, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4soby)\n",
        "    #CNN3DSy = tf.keras.layers.BatchNormalization()(conv4soby, training=True)\n",
        "\n",
        "    #xkir1 = tf.keras.layers.BatchNormalization()(xkir1, training=True)\n",
        "    print(\"filtro\")\n",
        "    conv1kir = tf.nn.relu(conv3d(xkir1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1kir)\n",
        "    conv1kir = tf.nn.max_pool3d(conv1kir, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1kir)\n",
        "    conv2kir = tf.nn.relu(conv3d(conv1kir, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2kir)\n",
        "    conv2kir = tf.nn.max_pool3d(conv2kir, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2kir)\n",
        "    conv3kir = tf.nn.relu(conv3d(conv2kir, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3kir)\n",
        "    conv3kir = tf.nn.max_pool3d(conv3kir, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3kir)\n",
        "    conv4kir = tf.nn.relu(conv3d(conv3kir, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4kir)\n",
        "    \n",
        "    \n",
        "    print(\"concatenacion\")\n",
        "    #conv = tf.concat([conv4gs, conv4sobx, conv4soby], -1)\n",
        "    conv = tf.multiply(conv4gs, conv4kir)\n",
        "    conv2 = tf.multiply(conv4sobx, conv4soby)\n",
        "    #conv = tf.multiply(conv, conv4soby)\n",
        "    print(conv)\n",
        "    \n",
        "    fc = tf.reshape(conv,[-1, cf])\n",
        "    fc = tf.nn.tanh(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "    fc = tf.nn.dropout(fc, rate=1-prob)\n",
        "    print(fc)\n",
        "    \n",
        "    fc1 = tf.nn.relu(tf.matmul(fc, weights['W_fc1'])+biases['b_fc1'])\n",
        "    fc1 = tf.nn.dropout(fc1, rate=1-prob)\n",
        "    print(fc1)\n",
        "    \n",
        "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['W_fc2'])+biases['b_fc2'])\n",
        "    fc2 = tf.nn.dropout(fc2, rate=1-prob)\n",
        "    print(fc2)\n",
        "    \n",
        "    fc3 = tf.nn.relu(tf.matmul(fc2, weights['W_fc3'])+biases['b_fc3'])\n",
        "    fc3 = tf.nn.dropout(fc3, rate=1-prob)\n",
        "    print(fc3)\n",
        "    \n",
        "    fc4 = tf.nn.relu(tf.matmul(fc3, weights['W_fc4'])+biases['b_fc4'])\n",
        "    fc4 = tf.nn.dropout(fc4, rate=1-prob)\n",
        "    print(fc4)\n",
        "    \n",
        "    fc5 = tf.nn.relu(tf.matmul(fc4, weights['W_fc5'])+biases['b_fc5'])\n",
        "    fc5 = tf.nn.dropout(fc5, rate=1-prob)\n",
        "    print(fc5)\n",
        "    \n",
        "    fc6 = tf.nn.relu(tf.matmul(fc5, weights['W_fc6'])+biases['b_fc6'])\n",
        "    fc6 = tf.nn.dropout(fc6, rate=1-prob)\n",
        "    print(fc6)\n",
        "    \n",
        "    fc7 = tf.nn.relu(tf.matmul(fc6, weights['W_fc7'])+biases['b_fc7'])\n",
        "    fc7 = tf.nn.dropout(fc7, rate=1-prob)\n",
        "    print(fc7)\n",
        "    \n",
        "    fc8 = tf.nn.relu(tf.matmul(fc7, weights['W_fc8'])+biases['b_fc8'])\n",
        "    fc8 = tf.nn.dropout(fc8, rate=1-prob)\n",
        "    print(fc8)\n",
        "    \n",
        "    fc9 = tf.nn.relu(tf.matmul(fc8, weights['W_fc9'])+biases['b_fc9'])\n",
        "    fc9 = tf.nn.dropout(fc9, rate=0)\n",
        "    print(fc9)\n",
        "    \n",
        "    output = tf.matmul(fc9, weights['out'])+biases['out']\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output))\n",
        "    #output = tf.nn.softmax(output)\n",
        "    #cost = tf.losses.mean_squared_error(labels=y, predictions=output)\n",
        "    \n",
        "    #output = tf.matmul(fc9, weights['out'])+biases['out']\n",
        "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output))\n",
        "    #output = tf.nn.softmax(output)\n",
        "    #cost = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=y, logits=output))\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
        "    #optimizer = tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)\n",
        "    \n",
        "    \n",
        "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(output,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
        "    \n",
        "    return dict(\n",
        "              x=x,\n",
        "              xr = xr,\n",
        "              xg = xg,\n",
        "              xb = xb,\n",
        "              xgs = xgs,\n",
        "              xsobx = xsobx,\n",
        "              xsoby = xsoby,\n",
        "              xlap = xlap,\n",
        "              xkir = xkir,\n",
        "              xoptx = xoptx,\n",
        "              xopty = xopty,\n",
        "              y=y,\n",
        "              prob = prob,\n",
        "              output=output,\n",
        "              cost=cost,\n",
        "              optimizer=optimizer,\n",
        "              accuracy = accuracy\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd6A4pO0ZTz0",
        "colab_type": "code",
        "outputId": "0b37f129-d71e-4a6c-80cd-cae0029eea38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "DNN=convolutional_neural_network(keep_rate = 0.5)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filtro\n",
            "Tensor(\"Relu:0\", shape=(?, 92, 48, 48, 6), dtype=float32)\n",
            "Tensor(\"MaxPool3D:0\", shape=(?, 46, 24, 24, 6), dtype=float32)\n",
            "Tensor(\"Relu_1:0\", shape=(?, 40, 22, 22, 16), dtype=float32)\n",
            "Tensor(\"MaxPool3D_1:0\", shape=(?, 20, 11, 11, 16), dtype=float32)\n",
            "Tensor(\"Relu_2:0\", shape=(?, 14, 9, 9, 32), dtype=float32)\n",
            "Tensor(\"MaxPool3D_2:0\", shape=(?, 7, 3, 3, 32), dtype=float32)\n",
            "Tensor(\"Relu_3:0\", shape=(?, 1, 1, 1, 128), dtype=float32)\n",
            "filtro\n",
            "Tensor(\"Relu_4:0\", shape=(?, 92, 48, 48, 6), dtype=float32)\n",
            "Tensor(\"MaxPool3D_3:0\", shape=(?, 46, 24, 24, 6), dtype=float32)\n",
            "Tensor(\"Relu_5:0\", shape=(?, 40, 22, 22, 16), dtype=float32)\n",
            "Tensor(\"MaxPool3D_4:0\", shape=(?, 20, 11, 11, 16), dtype=float32)\n",
            "Tensor(\"Relu_6:0\", shape=(?, 14, 9, 9, 32), dtype=float32)\n",
            "Tensor(\"MaxPool3D_5:0\", shape=(?, 7, 3, 3, 32), dtype=float32)\n",
            "Tensor(\"Relu_7:0\", shape=(?, 1, 1, 1, 128), dtype=float32)\n",
            "filtro\n",
            "Tensor(\"Relu_8:0\", shape=(?, 92, 48, 48, 6), dtype=float32)\n",
            "Tensor(\"MaxPool3D_6:0\", shape=(?, 46, 24, 24, 6), dtype=float32)\n",
            "Tensor(\"Relu_9:0\", shape=(?, 40, 22, 22, 16), dtype=float32)\n",
            "Tensor(\"MaxPool3D_7:0\", shape=(?, 20, 11, 11, 16), dtype=float32)\n",
            "Tensor(\"Relu_10:0\", shape=(?, 14, 9, 9, 32), dtype=float32)\n",
            "Tensor(\"MaxPool3D_8:0\", shape=(?, 7, 3, 3, 32), dtype=float32)\n",
            "Tensor(\"Relu_11:0\", shape=(?, 1, 1, 1, 128), dtype=float32)\n",
            "filtro\n",
            "Tensor(\"Relu_12:0\", shape=(?, 92, 48, 48, 6), dtype=float32)\n",
            "Tensor(\"MaxPool3D_9:0\", shape=(?, 46, 24, 24, 6), dtype=float32)\n",
            "Tensor(\"Relu_13:0\", shape=(?, 40, 22, 22, 16), dtype=float32)\n",
            "Tensor(\"MaxPool3D_10:0\", shape=(?, 20, 11, 11, 16), dtype=float32)\n",
            "Tensor(\"Relu_14:0\", shape=(?, 14, 9, 9, 32), dtype=float32)\n",
            "Tensor(\"MaxPool3D_11:0\", shape=(?, 7, 3, 3, 32), dtype=float32)\n",
            "Tensor(\"Relu_15:0\", shape=(?, 1, 1, 1, 128), dtype=float32)\n",
            "concatenacion\n",
            "Tensor(\"Mul:0\", shape=(?, 1, 1, 1, 128), dtype=float32)\n",
            "Tensor(\"dropout/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_1/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_2/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_3/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_4/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_5/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_6/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_7/mul_1:0\", shape=(?, 128), dtype=float32)\n",
            "Tensor(\"dropout_8/mul_1:0\", shape=(?, 64), dtype=float32)\n",
            "Tensor(\"Relu_24:0\", shape=(?, 64), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkAOfrpu5da6",
        "colab_type": "code",
        "outputId": "cb157b89-79ec-48ea-8dfb-aa4e4f7164ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "    print(\"filtroR\")\n",
        "    conv1r = tf.nn.relu(conv3d(xr1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1r)\n",
        "    conv1r = tf.nn.max_pool3d(conv1r, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1r)\n",
        "    conv2r = tf.nn.relu(conv3d(conv1r, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2r)\n",
        "    conv2r = tf.nn.max_pool3d(conv2r, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2r)\n",
        "    conv3r = tf.nn.relu(conv3d(conv2r, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3r)\n",
        "    conv3r = tf.nn.max_pool3d(conv3r, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3r)\n",
        "    conv4r = tf.nn.relu(conv3d(conv3r, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4r)\n",
        "    conv4r = tf.nn.max_pool3d(conv4r, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4r)\n",
        "    conv5r = tf.nn.relu(conv3d(conv4r, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5r)\n",
        "    conv6r = tf.nn.relu(conv3d(conv5r, weights['W_conv6']) + biases['b_conv6'])\n",
        "    print(con65r)\n",
        "    \n",
        "    print(\"filtroG\")\n",
        "    conv1g = tf.nn.relu(conv3d(xg1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1g)\n",
        "    conv1g = tf.nn.max_pool3d(conv1g, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1g)\n",
        "    conv2g = tf.nn.relu(conv3d(conv1g, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2g)\n",
        "    conv2g = tf.nn.max_pool3d(conv2g, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2g)\n",
        "    conv3g = tf.nn.relu(conv3d(conv2g, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3g)\n",
        "    conv3g = tf.nn.max_pool3d(conv3g, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3g)\n",
        "    conv4g = tf.nn.relu(conv3d(conv3g, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4g)\n",
        "    conv4g = tf.nn.max_pool3d(conv4g, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4g)\n",
        "    conv5g = tf.nn.relu(conv3d(conv4g, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5g)\n",
        "    \n",
        "    print(\"filtroB\")\n",
        "    conv1b = tf.nn.relu(conv3d(xb1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1b)\n",
        "    conv1b = tf.nn.max_pool3d(conv1b, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1b)\n",
        "    conv2b = tf.nn.relu(conv3d(conv1b, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2b)\n",
        "    conv2b = tf.nn.max_pool3d(conv2b, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2b)\n",
        "    conv3b = tf.nn.relu(conv3d(conv2b, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3b)\n",
        "    conv3b = tf.nn.max_pool3d(conv3b, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3b)\n",
        "    conv4b = tf.nn.relu(conv3d(conv3b, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4b)\n",
        "    conv4b = tf.nn.max_pool3d(conv4b, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4b)\n",
        "    conv5b = tf.nn.relu(conv3d(conv4b, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5b)\n",
        "    \n",
        "    \n",
        "    ##############################################################################################\n",
        "    \n",
        "  \n",
        "    print(\"filtro\")\n",
        "    conv1gs = tf.nn.relu(conv3d(xgs1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1gs)\n",
        "    conv1gs = tf.nn.max_pool3d(conv1gs, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1gs)\n",
        "    conv2gs = tf.nn.relu(conv3d(conv1gs, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2gs)\n",
        "    conv2gs = tf.nn.max_pool3d(conv2gs, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2gs)\n",
        "    conv3gs = tf.nn.relu(conv3d(conv2gs, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3gs)\n",
        "    conv3gs = tf.nn.max_pool3d(conv3gs, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3gs)\n",
        "    conv4gs = tf.nn.relu(conv3d(conv3gs, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4gs)\n",
        "    conv4gs = tf.nn.max_pool3d(conv4gs, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4gs)\n",
        "    conv5gs = tf.nn.relu(conv3d(conv4gs, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5gs)\n",
        "    conv6gs = tf.nn.relu(conv3d(conv5gs, weights['W_conv6']) + biases['b_conv6'])\n",
        "    print(conv6gs)\n",
        "\n",
        "    ##############################################################################################\n",
        "  \n",
        "    xsobx1 = tf.keras.layers.BatchNormalization()(xsobx1, training=True)\n",
        "    print(\"filtro\")\n",
        "    conv1sobx = tf.nn.relu(conv3d(xsobx1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1sobx)\n",
        "    conv1sobx = tf.nn.max_pool3d(conv1sobx, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1sobx)\n",
        "    conv2sobx = tf.nn.relu(conv3d(conv1sobx, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2sobx)\n",
        "    conv2sobx = tf.nn.max_pool3d(conv2sobx, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2sobx)\n",
        "    conv3sobx = tf.nn.relu(conv3d(conv2sobx, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3sobx)\n",
        "    conv3sobx = tf.nn.max_pool3d(conv3sobx, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3sobx)\n",
        "    conv4sobx = tf.nn.relu(conv3d(conv3sobx, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4sobx)\n",
        "    #CNN3DSX = tf.keras.layers.BatchNormalization()(conv4sobx, training=True)\n",
        "    \n",
        "    xsoby1 = tf.keras.layers.BatchNormalization()(xsoby1, training=True)\n",
        "    print(\"filtro\")\n",
        "    conv1soby = tf.nn.relu(conv3d(xsoby1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1soby)\n",
        "    conv1soby = tf.nn.max_pool3d(conv1soby, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1soby)\n",
        "    conv2soby = tf.nn.relu(conv3d(conv1soby, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2soby)\n",
        "    conv2soby = tf.nn.max_pool3d(conv2soby, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2soby)\n",
        "    conv3soby = tf.nn.relu(conv3d(conv2soby, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3soby)\n",
        "    conv3soby = tf.nn.max_pool3d(conv3soby, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3soby)\n",
        "    conv4soby = tf.nn.relu(conv3d(conv3soby, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4soby)\n",
        "    #CNN3DSy = tf.keras.layers.BatchNormalization()(conv4soby, training=True)\n",
        "    ###############################################################################################\n",
        "    \n",
        "    print(\"filtro\")\n",
        "    conv1lap = tf.nn.relu(conv3d(xlap1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1lap)\n",
        "    conv1lap = tf.nn.max_pool3d(conv1lap, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1lap)\n",
        "    conv2lap = tf.nn.relu(conv3d(conv1lap, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2lap)\n",
        "    conv2lap = tf.nn.max_pool3d(conv2lap, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2lap)\n",
        "    conv3lap = tf.nn.relu(conv3d(conv2lap, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3lap)\n",
        "    conv3lap = tf.nn.max_pool3d(conv3lap, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3lap)\n",
        "    conv4lap = tf.nn.relu(conv3d(conv3lap, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4lap)\n",
        "    conv4lap = tf.nn.max_pool3d(conv4lap, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4lap)\n",
        "    conv5lap = tf.nn.relu(conv3d(conv4lap, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5lap)\n",
        "    \n",
        "    print(\"filtro\")\n",
        "    conv1kir = tf.nn.relu(conv3d(xkir1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1kir)\n",
        "    conv1kir = tf.nn.max_pool3d(conv1kir, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1kir)\n",
        "    conv2kir = tf.nn.relu(conv3d(conv1kir, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2kir)\n",
        "    conv2kir = tf.nn.max_pool3d(conv2kir, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2kir)\n",
        "    conv3kir = tf.nn.relu(conv3d(conv2kir, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3kir)\n",
        "    conv3kir = tf.nn.max_pool3d(conv3kir, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3kir)\n",
        "    conv4kir = tf.nn.relu(conv3d(conv3kir, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4kir)\n",
        "    conv4kir = tf.nn.max_pool3d(conv4kir, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4kir)\n",
        "    conv5kir = tf.nn.relu(conv3d(conv4kir, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5kir)\n",
        "    conv6kir = tf.nn.relu(conv3d(conv5kir, weights['W_conv6']) + biases['b_conv6'])\n",
        "    print(conv6kir)\n",
        "    \n",
        "    ################################################################################################\n",
        "    print(\"filtro\")\n",
        "    conv1optx = tf.nn.relu(conv3d(xoptx1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1optx)\n",
        "    conv1optx = tf.nn.max_pool3d(conv1optx, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1optx)\n",
        "    conv2optx = tf.nn.relu(conv3d(conv1optx, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2optx)\n",
        "    conv2optx = tf.nn.max_pool3d(conv2optx, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2optx)\n",
        "    conv3optx = tf.nn.relu(conv3d(conv2optx, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3optx)\n",
        "    conv3optx = tf.nn.max_pool3d(conv3optx, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3optx)\n",
        "    conv4optx = tf.nn.relu(conv3d(conv3optx, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4optx)\n",
        "    conv4optx = tf.nn.max_pool3d(conv4optx, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4optx)\n",
        "    conv5optx = tf.nn.relu(conv3d(conv4optx, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5optx)\n",
        "    \n",
        "    print(\"filtro\")\n",
        "    conv1opty = tf.nn.relu(conv3d(xopty1, weights['W_conv1']) + biases['b_conv1'])\n",
        "    print(conv1opty)\n",
        "    conv1opty = tf.nn.max_pool3d(conv1opty, ksize=maxpool1, strides=maxpool1, padding='SAME')\n",
        "    print(conv1opty)\n",
        "    conv2opty = tf.nn.relu(conv3d(conv1opty, weights['W_conv2']) + biases['b_conv2'])\n",
        "    print(conv2opty)\n",
        "    conv2opty = tf.nn.max_pool3d(conv2opty, ksize=maxpool2, strides=maxpool2, padding='SAME')\n",
        "    print(conv2opty)\n",
        "    conv3opty = tf.nn.relu(conv3d(conv2opty, weights['W_conv3']) + biases['b_conv3'])\n",
        "    print(conv3opty)\n",
        "    conv3opty = tf.nn.max_pool3d(conv3opty, ksize=maxpool3, strides=maxpool3, padding='VALID')\n",
        "    print(conv3opty)\n",
        "    conv4opty = tf.nn.relu(conv3d(conv3opty, weights['W_conv4']) + biases['b_conv4'])\n",
        "    print(conv4opty)\n",
        "    conv4opty = tf.nn.max_pool3d(conv4opty, ksize=maxpool4, strides=maxpool4, padding='VALID')\n",
        "    print(conv4opty)\n",
        "    conv5opty = tf.nn.relu(conv3d(conv4opty, weights['W_conv5']) + biases['b_conv5'])\n",
        "    print(conv5opty)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "filtroR\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-572b4a36417e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filtroR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mconv1r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W_conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b_conv1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mconv1r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'xr1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAawl5y0LJ-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feed_dict={DNN[\"xr\"]:   dataTrainR[start:end],\n",
        "           DNN[\"xg\"]: dataTrainG[start:end],\n",
        "           DNN[\"xb\"]: dataTrainB[start:end],\n",
        "           DNN[\"xgs\"]:   dataTrainGS[start:end],\n",
        "           DNN[\"xsobx\"]: dataTrainSX[start:end],\n",
        "           DNN[\"xsoby\"]: dataTrainSY[start:end],\n",
        "           DNN[\"xlap\"]: dataTrainLAP[start:end],\n",
        "           DNN[\"xkir\"]: dataTrainKIR[start:end],\n",
        "           DNN[\"xoptx\"]: dataTrainOX[start:end],\n",
        "           DNN[\"xopty\"]: dataTrainOY[start:end],\n",
        "           DNN[\"y\"]:     dataTrainY[start:end],\n",
        "           DNN[\"prob\"]: 0.5} #antes era 45\n",
        "\n",
        "feed_dict={DNN[\"xr\"]: dataTestR[x],\n",
        "           DNN[\"xg\"]: dataTestG[x],\n",
        "           DNN[\"xb\"]: dataTestB[x],\n",
        "           DNN[\"xgs\"]:   dataTestGS[x],\n",
        "           DNN[\"xsobx\"]: dataTestSX[x],\n",
        "           DNN[\"xsoby\"]: dataTestSY[x],\n",
        "           DNN[\"xlap\"]: dataTestLAP[x],\n",
        "           DNN[\"xkir\"]: dataTestKIR[x],\n",
        "           DNN[\"xoptx\"]: dataTestOX[x],\n",
        "           DNN[\"xopty\"]: dataTestOY[x],\n",
        "           DNN[\"y\"]:     dataTestY[x],\n",
        "           DNN[\"prob\"]: 0.5} #antes era 45"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHCU8Gt8RsrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_neural_network(DNN, hm_epochs = 10,batch_size=70):\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        #saver.restore(sess,save_file)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        \n",
        "        golfa = 1\n",
        "        bestWeights = 0\n",
        "         ########################################################################## training\n",
        "        \n",
        "        for epoch in range(hm_epochs):        \n",
        "          \n",
        "            epoch_loss = 0\n",
        "            epoch_lossVal = 0\n",
        "            i=0\n",
        "            acctrain = 0\n",
        "            contador = 0\n",
        "            while i < len(dataTrainY)/batch_size:\n",
        "                start = i\n",
        "                end = i+batch_size\n",
        "\n",
        "                feed_dict={DNN[\"xkir\"]: dataTrainKIR[start:end],\n",
        "                           DNN[\"xgs\"]:   dataTrainGS[start:end],\n",
        "                           DNN[\"xsobx\"]: dataTrainSX[start:end],\n",
        "                           DNN[\"xsoby\"]: dataTrainSY[start:end],\n",
        "                           DNN[\"y\"]:    dataTrainY[start:end],\n",
        "                           DNN[\"prob\"]: golfa} #antes era 45\n",
        "                _, c, accuracy  = sess.run([DNN[\"optimizer\"], DNN[\"cost\"]    # _, c, output\n",
        "                                                 , DNN[\"accuracy\"]], #DNN[\"output\"]\n",
        "                                                feed_dict=feed_dict)\n",
        "                acctrain = acctrain + accuracy\n",
        "                \n",
        "                epoch_loss += c\n",
        "                \n",
        "                contador = contador + 1\n",
        "                i+=batch_size\n",
        "            \n",
        "            \n",
        "            \n",
        "             ########################################################################## Validation\n",
        "            \n",
        "            \n",
        "            \n",
        "            feed_dict={DNN[\"xkir\"]: dataTrainKIR2,\n",
        "                       DNN[\"xgs\"]:   dataTrainGS2,\n",
        "                       DNN[\"xsobx\"]: dataTrainSX2,\n",
        "                       DNN[\"xsoby\"]: dataTrainSY2,\n",
        "                       DNN[\"y\"]:     dataTrainY2,\n",
        "                       DNN[\"prob\"]: 1} #antes era 45\n",
        "            \n",
        "            accuracyVal,cVal  = sess.run([DNN[\"accuracy\"],DNN[\"cost\"]],feed_dict=feed_dict)\n",
        "            epoch_lossVal += cVal\n",
        "                \n",
        "            #print('\\n\\nEpoch', epoch, 'completed out of',hm_epochs)\n",
        "            #print('LossTraining:\\t',epoch_loss,'lossValidation:\\t',epoch_lossVal)\n",
        "            #print('Train: \\t  accuracyValidation:\\t',accuracyVal, '\\t accuracyTraining:\\t',acctrain/contador)\n",
        "            \n",
        "            if((acctrain/contador) > 0.6 and (acctrain/contador) < 0.8):\n",
        "              golfa = 0.9\n",
        "            elif((acctrain/contador) > 0.8):\n",
        "              golfa = 0.8\n",
        "            else:\n",
        "              golfa = 1\n",
        "            \n",
        "            \n",
        "            ########################################################################## impresion tensorboard training validation\n",
        "            \n",
        "            \n",
        "            tbc.save_value(\"LossTrain\", \"lossTrain\", epoch, epoch_loss)\n",
        "            tbc.save_value(\"AccuracyTrain\", \"accuracyTrain\", epoch, acctrain/contador)\n",
        "            \n",
        "            tbc.save_value(\"LossTrain\", \"lossValidation\", epoch, epoch_lossVal)\n",
        "            tbc.save_value(\"AccuracyTrain\", \"accuracyValidation\", epoch, accuracyVal)\n",
        "            \n",
        "            tbc.flush_line(\"lossTrain\")\n",
        "            tbc.flush_line(\"accuracyTrain\")\n",
        "            tbc.flush_line(\"lossValidation\")\n",
        "            tbc.flush_line(\"accuracyValidation\")\n",
        "\n",
        "            \n",
        "            \n",
        "            \n",
        "            ########################################################################## test uno (personas que nunca se vieron)\n",
        "            \n",
        "            if((epoch+1)%1 == 0):\n",
        "              \n",
        "                golfita1 = 0 #variable de impresion\n",
        "                golfita11 = 0  #variable de impresion\n",
        "                \n",
        "                \n",
        "                acctest1 = 0\n",
        "                \n",
        "                accumaccuracy = 0\n",
        "                epoch_lossTest = 0\n",
        "                contadorTotal = 0\n",
        "                contadorTotalcorrecto = 0\n",
        "                \n",
        "                contadorVerdadPositiva = 0\n",
        "                contadorVerdadNegativa = 0\n",
        "                contadorMentiraPositiva = 0\n",
        "                contadorMentiraNegativa = 0\n",
        "                for x in range(len(dataTestY)):\n",
        "                  if(len(dataTestGS[x]) != 0 ):\n",
        "                    feed_dict={DNN[\"xkir\"]: dataTestKIR[x],\n",
        "                               DNN[\"xgs\"]:   dataTestGS[x],\n",
        "                               DNN[\"xsobx\"]: dataTestSX[x],\n",
        "                               DNN[\"xsoby\"]: dataTestSY[x],\n",
        "                               DNN[\"y\"]:     dataTestY[x],\n",
        "                               DNN[\"prob\"]: 1} #antes era 45\n",
        "                    predictionTest,yTest,accuracyTest,cTest   = sess.run([DNN[\"output\"], DNN[\"y\"], DNN[\"accuracy\"],DNN[\"cost\"]],\n",
        "                                                                         feed_dict=feed_dict)\n",
        "                    \n",
        "                    epoch_lossTest += cTest\n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    contadorCorrecto = 0\n",
        "                    contadorIncorrecto = 0\n",
        "                    \n",
        "                    for i in range(len(dataTestY[x])):\n",
        "                        if((predictionTest[i][0]>predictionTest[i][1] and yTest[i][0]>yTest[i][1]) or\n",
        "                           (predictionTest[i][0]<predictionTest[i][1] and yTest[i][0]<yTest[i][1])):\n",
        "                          \n",
        "                          contadorCorrecto = contadorCorrecto + 1\n",
        "                          \n",
        "                          if(x > 20):\n",
        "                            \n",
        "                            if((x+1) % 2 == 0):\n",
        "                              contadorMentiraPositiva += 1\n",
        "                            else:\n",
        "                              contadorMentiraNegativa += 1\n",
        "                              \n",
        "                          else:\n",
        "                            \n",
        "                            if((x+1) % 2 == 0):\n",
        "                              contadorVerdadPositiva += 1\n",
        "                            else:\n",
        "                              contadorVerdadNegativa += 1\n",
        "                              \n",
        "                        else:\n",
        "                          contadorIncorrecto = contadorIncorrecto + 1\n",
        "                          \n",
        "                        \n",
        "                        \n",
        "                    if(contadorCorrecto >  contadorIncorrecto):\n",
        "                      accumaccuracy = accumaccuracy + 1\n",
        "                    contadorTotal = contadorTotal + contadorCorrecto + contadorIncorrecto\n",
        "                    contadorTotalcorrecto = contadorTotalcorrecto + contadorCorrecto\n",
        "                    \n",
        "                acctest1 = contadorTotalcorrecto/contadorTotal\n",
        "                \n",
        "                #print('Test1: \\t epoch_lossTest1 \\t',epoch_lossTest)\n",
        "                #print('Test1: \\t accuracyPorVideoTest1 \\t',accumaccuracy/len(dataTestY),'\\t accuracyGlobal \\t',acctest1)\n",
        "                #print('Test1: \\t contadorMentiraPositiva \\t',(contadorMentiraPositiva/contadorTotal)/acctest1,'\\t contadorMentiraNegativa \\t',(contadorMentiraNegativa/contadorTotal)/acctest1)\n",
        "                #print('Test1: \\t contadorVerdadPositiva \\t',(contadorVerdadPositiva/contadorTotal)/acctest1,'\\t contadorVerdadNegativa \\t',(contadorVerdadNegativa/contadorTotal)/acctest1)\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "                ########################################################################## impresion tensorboard test1\n",
        "                \n",
        "                if(golfita1 <= acctest1):\n",
        "                  golfita1 = acctest1\n",
        "                elif(acctest1 > golfita1*2/3):\n",
        "                  golfita1 = acctest1\n",
        "                  \n",
        "                if(golfita11 <= accumaccuracy/len(dataTestY)):\n",
        "                  golfita11 = accumaccuracy/len(dataTestY)\n",
        "                elif(accumaccuracy/len(dataTestY) > golfita11*2/3):\n",
        "                  golfita11 = accumaccuracy/len(dataTestY)\n",
        "                \n",
        "                \n",
        "                #tbc.save_value(\"AccuracyTest\", \"accuracyGlobal1\", epoch, golfita1)\n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyGlobal1\", epoch, acctest1)\n",
        "                tbc.flush_line(\"accuracyGlobal1\")\n",
        "                \n",
        "                tbc.save_value(\"LossTest\", \"lossTest\", epoch, epoch_lossTest)\n",
        "                #tbc.save_value(\"AccuracyTest\", \"accuracyPorVideo1\", epoch, golfita11)\n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyPorVideo1\", epoch, accumaccuracy/len(dataTestY))\n",
        "                tbc.flush_line(\"lossTest\")\n",
        "                \n",
        "                ######################################################################### Test dos (personas ya vistas) verdad\n",
        "                \n",
        "                \n",
        "                golfita2 = 0  #variable de impresion\n",
        "                golfita22 = 0  #variable de impresion\n",
        "                \n",
        "                \n",
        "                acctest2 = 0   \n",
        "                   \n",
        "                accumaccuracy = 0\n",
        "                epoch_lossTest = 0\n",
        "                contadorTotal = 0\n",
        "                contadorTotalcorrecto = 0\n",
        "                   \n",
        "                   \n",
        "                contadorVerdadNegativa = 0\n",
        "                contadorMentiraPositiva = 0\n",
        "                   \n",
        "                acctotal = 0\n",
        "                   \n",
        "                for x in range(len(dataTestTruthY2)+len(dataTestDeceitY2)):\n",
        "                  if((x+1) < len(dataTestTruthY2)):\n",
        "                    feed_dict={DNN[\"xkir\"]: dataTestTruthKIR2[x],\n",
        "                               DNN[\"xgs\"]:   dataTestTruthGS2[x],\n",
        "                               DNN[\"xsobx\"]: dataTestTruthSX2[x],\n",
        "                               DNN[\"xsoby\"]: dataTestTruthSY2[x],\n",
        "                               DNN[\"y\"]:     dataTestTruthY2[x],\n",
        "                               DNN[\"prob\"]: 1} #antes era 45\n",
        "                    predictionTest,yTest,accuracyTest,cTest   = sess.run([DNN[\"output\"], DNN[\"y\"], DNN[\"accuracy\"],DNN[\"cost\"]],\n",
        "                                                                         feed_dict=feed_dict)\n",
        "                    epoch_lossTest += cTest\n",
        "                    \n",
        "                    contadorCorrecto = 0\n",
        "                    contadorIncorrecto = 0\n",
        "                   \n",
        "                    for i in range(len(dataTestTruthY2[x])):\n",
        "                        if((predictionTest[i][0]>predictionTest[i][1] and yTest[i][0]>yTest[i][1]) or\n",
        "                           (predictionTest[i][0]<predictionTest[i][1] and yTest[i][0]<yTest[i][1])):\n",
        "                          contadorCorrecto = contadorCorrecto + 1\n",
        "                        else:\n",
        "                          contadorIncorrecto = contadorIncorrecto + 1\n",
        "                    if(contadorCorrecto >  contadorIncorrecto):\n",
        "                      accumaccuracy = accumaccuracy + 1\n",
        "                      contadorVerdadNegativa += 1\n",
        "                   \n",
        "                    contadorTotal = contadorTotal + contadorCorrecto + contadorIncorrecto\n",
        "                    contadorTotalcorrecto = contadorTotalcorrecto + contadorCorrecto\n",
        "                    \n",
        "                  else:\n",
        "                    feed_dict={DNN[\"xkir\"]: dataTestDeceitKIR2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"xgs\"]:   dataTestDeceitGS2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"xsobx\"]: dataTestDeceitSX2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"xsoby\"]: dataTestDeceitSY2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"y\"]:     dataTestDeceitY2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"prob\"]: 1} #antes era 45\n",
        "                    predictionTest,yTest,accuracyTest,cTest   = sess.run([DNN[\"output\"], DNN[\"y\"], DNN[\"accuracy\"],DNN[\"cost\"]],\n",
        "                                                                         feed_dict=feed_dict)\n",
        "                    \n",
        "                    epoch_lossTest += cTest\n",
        "                    \n",
        "                    contadorCorrecto = 0\n",
        "                    contadorIncorrecto = 0\n",
        "                   \n",
        "                    for i in range(len(dataTestDeceitY2[x-len(dataTestTruthY2)])):\n",
        "                        #print(len(dataTestDeceitY2[x-len(dataTestTruthY2)-1]))\n",
        "                        #print(i)\n",
        "                        if((predictionTest[i][0]>predictionTest[i][1] and yTest[i][0]>yTest[i][1]) or\n",
        "                           (predictionTest[i][0]<predictionTest[i][1] and yTest[i][0]<yTest[i][1])):\n",
        "                          contadorCorrecto = contadorCorrecto + 1\n",
        "                          #print(contadorCorrecto)\n",
        "                        else:\n",
        "                          contadorIncorrecto = contadorIncorrecto + 1\n",
        "                    if(contadorCorrecto >  contadorIncorrecto):\n",
        "                      accumaccuracy = accumaccuracy + 1\n",
        "                      contadorMentiraPositiva += 1\n",
        "                    contadorTotal = contadorTotal + contadorCorrecto + contadorIncorrecto\n",
        "                    contadorTotalcorrecto = contadorTotalcorrecto + contadorCorrecto\n",
        "                    \n",
        "                #print('Test2: \\t epoch_lossTest2 \\t',epoch_lossTest)\n",
        "                #print('Test2: \\t accuracyPorVideo2 \\t',accumaccuracy/(len(dataTestDeceitY2)+len(dataTestTruthY2)),'\\t accuracyGlobal2 \\t',contadorTotalcorrecto/contadorTotal)\n",
        "                #print('Test2: \\t contadorMentiraPositiva \\t',contadorMentiraPositiva/len(dataTestDeceitY2),'\\t contadorVerdadNegativa \\t',contadorVerdadNegativa/len(dataTestTruthY2))\n",
        "                \n",
        "                \n",
        "                \n",
        "                ########################################################################## impresion tensorboard test2\n",
        "                \n",
        "                \n",
        "                if(golfita2 <= contadorTotalcorrecto/contadorTotal):\n",
        "                  golfita2 = contadorTotalcorrecto/contadorTotal\n",
        "                elif(contadorTotalcorrecto/contadorTotal > golfita2*2/3):\n",
        "                  golfita2 = contadorTotalcorrecto/contadorTotal\n",
        "                  \n",
        "                if(golfita22 <= accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))):\n",
        "                  golfita22 = accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))\n",
        "                elif(accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2)) > golfita22*2/3):\n",
        "                  golfita22 = accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))\n",
        "                \n",
        "                #tbc.save_value(\"AccuracyTest\", \"accuracyGlobal2\", epoch, golfita2)\n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyGlobal2\", epoch, contadorTotalcorrecto/contadorTotal)\n",
        "                tbc.flush_line(\"accuracyGlobal2\")\n",
        "                \n",
        "                tbc.save_value(\"LossTest\", \"lossTest2\", epoch, epoch_lossTest)\n",
        "                #tbc.save_value(\"AccuracyTest\", \"accuracyPorVideo2\", epoch, golfita22)\n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyPorVideo2\", epoch, accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2)))\n",
        "                tbc.flush_line(\"lossTest2\")\n",
        "                \n",
        "                roar = contadorTotalcorrecto/contadorTotal\n",
        "                \n",
        "                ########################################################################## condicion para guardar los pesos de la red\n",
        "                \n",
        "                if(roar > 0.60 and roar > bestWeights and acctrain/contador > 0.60):\n",
        "                  saver.save(sess,save_file)\n",
        "                  bestWeights = roar\n",
        "                  print('\\n\\nEpoch', epoch, 'completed out of',hm_epochs)\n",
        "                  print('LossTraining:\\t',epoch_loss,'lossValidation:\\t',epoch_lossVal)\n",
        "                  print('Train: \\t  accuracyValidation:\\t',accuracyVal, '\\t accuracyTraining:\\t',acctrain/contador)\n",
        "                  print('Test2: \\t epoch_lossTest2 \\t',epoch_lossTest)\n",
        "                  print('Test2: \\t accuracyPorVideo2 \\t',accumaccuracy/(len(dataTestDeceitY2)+len(dataTestTruthY2)),'\\t accuracyGlobal2 \\t',contadorTotalcorrecto/contadorTotal)\n",
        "                  print('Test2: \\t contadorMentiraPositiva \\t',contadorMentiraPositiva/len(dataTestDeceitY2),'\\t contadorVerdadNegativa \\t',contadorVerdadNegativa/len(dataTestTruthY2))\n",
        "\n",
        "        tbc.close()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8Rdd_Q3ZUa-",
        "colab_type": "code",
        "outputId": "232316e5-7eb4-4074-925d-c946038491a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "tbc=TensorBoardColab()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://bbb4c63d.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "z9GgiaCote_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_neural_network(DNN,hm_epochs = 4000,batch_size=150)\n",
        "#LEL\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV9IsukLakJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in range(len(dataTestDeceitY2)):\n",
        "    print(dataTestDeceitY2[x])\n",
        "    for y in range(len(dataTestDeceitGS2[x])):\n",
        "      plt.imshow(dataTestDeceitGS2[x][y][0], interpolation='none', cmap='gray')\n",
        "      plt.xticks([]), plt.yticks([])\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1eH_Qrtte_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_neural_network(DNN, hm_epochs = 10,batch_size=70):\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        #saver.restore(sess,save_file)\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "         ########################################################################## training\n",
        "        \n",
        "        for epoch in range(hm_epochs):        \n",
        "          \n",
        "            epoch_loss = 0\n",
        "            epoch_lossVal = 0\n",
        "            i=0\n",
        "            acctrain = 0\n",
        "            contador = 0\n",
        "            while i < len(dataTrainY)/batch_size:\n",
        "                start = i\n",
        "                end = i+batch_size\n",
        "\n",
        "                feed_dict={DNN[\"xkir\"]: dataTrainKIR[start:end],\n",
        "                           DNN[\"xgs\"]:   dataTrainGS[start:end],\n",
        "                           #DNN[\"xsobx\"]: dataTrainSX[start:end],\n",
        "                           #DNN[\"xsoby\"]: dataTrainSY[start:end],\n",
        "                           DNN[\"y\"]:    dataTrainY[start:end],\n",
        "                           DNN[\"prob\"]: 1} #antes era 45\n",
        "                _, c, accuracy  = sess.run([DNN[\"optimizer\"], DNN[\"cost\"]    # _, c, output\n",
        "                                                 , DNN[\"accuracy\"]], #DNN[\"output\"]\n",
        "                                                feed_dict=feed_dict)\n",
        "                acctrain = acctrain + accuracy\n",
        "                \n",
        "                epoch_loss += c\n",
        "                \n",
        "                contador = contador + 1\n",
        "                i+=batch_size\n",
        "            \n",
        "            \n",
        "            \n",
        "             ########################################################################## Validation\n",
        "            \n",
        "            \n",
        "            \n",
        "            feed_dict={DNN[\"xkir\"]: dataTrainKIR2,\n",
        "                       DNN[\"xgs\"]:   dataTrainGS2,\n",
        "                       #DNN[\"xsobx\"]: dataTrainSX2,\n",
        "                       #DNN[\"xsoby\"]: dataTrainSY2,\n",
        "                       DNN[\"y\"]:     dataTrainY2,\n",
        "                       DNN[\"prob\"]: 1} #antes era 45\n",
        "            \n",
        "            accuracyVal,cVal  = sess.run([DNN[\"accuracy\"],DNN[\"cost\"]],feed_dict=feed_dict)\n",
        "            epoch_lossVal += cVal\n",
        "                \n",
        "            print('\\n\\nEpoch', epoch, 'completed out of',hm_epochs)\n",
        "            print('LossTraining:\\t',epoch_loss,'lossValidation:\\t',epoch_lossVal)\n",
        "            print('Train: \\t  accuracyValidation:\\t',accuracyVal, '\\t accuracyTraining:\\t',acctrain/contador)\n",
        "            \n",
        "   \n",
        "            \n",
        "            \n",
        "            ########################################################################## impresion tensorboard training validation\n",
        "            \n",
        "            \n",
        "            tbc.save_value(\"LossTrain\", \"lossTrain\", epoch, epoch_loss)\n",
        "            tbc.save_value(\"AccuracyTrain\", \"accuracyTrain\", epoch, acctrain/contador)\n",
        "            \n",
        "            tbc.save_value(\"LossTrain\", \"lossValidation\", epoch, epoch_lossVal)\n",
        "            tbc.save_value(\"AccuracyTrain\", \"accuracyValidation\", epoch, accuracyVal)\n",
        "            \n",
        "            tbc.flush_line(\"lossTrain\")\n",
        "            tbc.flush_line(\"accuracyTrain\")\n",
        "            tbc.flush_line(\"lossValidation\")\n",
        "            tbc.flush_line(\"accuracyValidation\")\n",
        "\n",
        "            \n",
        "            \n",
        "            if((epoch+1)%1 == 0):\n",
        "              \n",
        "                \n",
        "                \n",
        "                ######################################################################### Test dos (personas ya vistas) verdad\n",
        "                \n",
        "                \n",
        "                golfita2 = 0  #variable de impresion\n",
        "                golfita22 = 0  #variable de impresion\n",
        "                \n",
        "                \n",
        "                acctest2 = 0   \n",
        "                   \n",
        "                accumaccuracy = 0\n",
        "                epoch_lossTest = 0\n",
        "                contadorTotal = 0\n",
        "                contadorTotalcorrecto = 0\n",
        "                   \n",
        "                   \n",
        "                contadorVerdadNegativa = 0\n",
        "                contadorMentiraPositiva = 0\n",
        "                   \n",
        "                acctotal = 0\n",
        "                   \n",
        "                for x in range(len(dataTestTruthY2)+len(dataTestDeceitY2)):\n",
        "                  if((x+1) < len(dataTestTruthY2)):\n",
        "                    feed_dict={DNN[\"xkir\"]: dataTestTruthKIR2[x],\n",
        "                               DNN[\"xgs\"]:   dataTestTruthGS2[x],\n",
        "                               #DNN[\"xsobx\"]: dataTestTruthSX2[x],\n",
        "                               #DNN[\"xsoby\"]: dataTestTruthSY2[x],\n",
        "                               DNN[\"y\"]:     dataTestTruthY2[x],\n",
        "                               DNN[\"prob\"]: 1} #antes era 45\n",
        "                    predictionTest,yTest,accuracyTest,cTest   = sess.run([DNN[\"output\"], DNN[\"y\"], DNN[\"accuracy\"],DNN[\"cost\"]],\n",
        "                                                                         feed_dict=feed_dict)\n",
        "                    epoch_lossTest += cTest\n",
        "                    \n",
        "                    contadorCorrecto = 0\n",
        "                    contadorIncorrecto = 0\n",
        "                   \n",
        "                    for i in range(len(dataTestTruthY2[x])):\n",
        "                        if((predictionTest[i][0]>predictionTest[i][1] and yTest[i][0]>yTest[i][1]) or\n",
        "                           (predictionTest[i][0]<predictionTest[i][1] and yTest[i][0]<yTest[i][1])):\n",
        "                          contadorCorrecto = contadorCorrecto + 1\n",
        "                        else:\n",
        "                          contadorIncorrecto = contadorIncorrecto + 1\n",
        "                    if(contadorCorrecto >  contadorIncorrecto):\n",
        "                      accumaccuracy = accumaccuracy + 1\n",
        "                      contadorVerdadNegativa += 1\n",
        "                   \n",
        "                    contadorTotal = contadorTotal + contadorCorrecto + contadorIncorrecto\n",
        "                    contadorTotalcorrecto = contadorTotalcorrecto + contadorCorrecto\n",
        "                    \n",
        "                  else:\n",
        "                    feed_dict={DNN[\"xkir\"]: dataTestDeceitKIR2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"xgs\"]:   dataTestDeceitGS2[x-len(dataTestTruthY2)],\n",
        "                               #DNN[\"xsobx\"]: dataTestDeceitSX2[x-len(dataTestTruthY2)],\n",
        "                               #DNN[\"xsoby\"]: dataTestDeceitSY2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"y\"]:     dataTestDeceitY2[x-len(dataTestTruthY2)],\n",
        "                               DNN[\"prob\"]: 1} #antes era 45\n",
        "                    predictionTest,yTest,accuracyTest,cTest   = sess.run([DNN[\"output\"], DNN[\"y\"], DNN[\"accuracy\"],DNN[\"cost\"]],\n",
        "                                                                         feed_dict=feed_dict)\n",
        "                    \n",
        "                    epoch_lossTest += cTest\n",
        "                    \n",
        "                    contadorCorrecto = 0\n",
        "                    contadorIncorrecto = 0\n",
        "                   \n",
        "                    for i in range(len(dataTestDeceitY2[x-len(dataTestTruthY2)])):\n",
        "                        #print(len(dataTestDeceitY2[x-len(dataTestTruthY2)-1]))\n",
        "                        #print(i)\n",
        "                        if((predictionTest[i][0]>predictionTest[i][1] and yTest[i][0]>yTest[i][1]) or\n",
        "                           (predictionTest[i][0]<predictionTest[i][1] and yTest[i][0]<yTest[i][1])):\n",
        "                          contadorCorrecto = contadorCorrecto + 1\n",
        "                          #print(contadorCorrecto)\n",
        "                        else:\n",
        "                          contadorIncorrecto = contadorIncorrecto + 1\n",
        "                    if(contadorCorrecto >  contadorIncorrecto):\n",
        "                      accumaccuracy = accumaccuracy + 1\n",
        "                      contadorMentiraPositiva += 1\n",
        "                    contadorTotal = contadorTotal + contadorCorrecto + contadorIncorrecto\n",
        "                    contadorTotalcorrecto = contadorTotalcorrecto + contadorCorrecto\n",
        "                    \n",
        "                print('Test2: \\t epoch_lossTest2 \\t',epoch_lossTest)\n",
        "                print('Test2: \\t accuracyPorVideo2 \\t',accumaccuracy/(len(dataTestDeceitY2)+len(dataTestTruthY2)),'\\t accuracyGlobal2 \\t',contadorTotalcorrecto/contadorTotal)\n",
        "                print('Test2: \\t contadorMentiraPositiva \\t',contadorMentiraPositiva/len(dataTestDeceitY2),'\\t contadorVerdadNegativa \\t',contadorVerdadNegativa/len(dataTestTruthY2))\n",
        "                \n",
        "                \n",
        "                \n",
        "                ########################################################################## impresion tensorboard test2\n",
        "                \n",
        "                \n",
        "                if(golfita2 <= contadorTotalcorrecto/contadorTotal):\n",
        "                  golfita2 = contadorTotalcorrecto/contadorTotal\n",
        "                elif(contadorTotalcorrecto/contadorTotal > golfita2*2/3):\n",
        "                  golfita2 = contadorTotalcorrecto/contadorTotal\n",
        "                  \n",
        "                if(golfita22 <= accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))):\n",
        "                  golfita22 = accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))\n",
        "                elif(accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2)) > golfita22*2/3):\n",
        "                  golfita22 = accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2))\n",
        "                \n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyGlobal2\", epoch, golfita2)\n",
        "                tbc.flush_line(\"accuracyGlobal2\")\n",
        "                \n",
        "                tbc.save_value(\"LossTest\", \"lossTest2\", epoch, epoch_lossTest)\n",
        "                tbc.save_value(\"AccuracyTest\", \"accuracyPorVideo2\", epoch, golfita22)\n",
        "                tbc.flush_line(\"lossTest2\")\n",
        "                \n",
        "                \n",
        "                ########################################################################## condicion para guardar los pesos de la red\n",
        "                \n",
        "                if(accumaccuracy/(len(dataTestTruthY2)+len(dataTestDeceitY2)) > 0.70):\n",
        "                  saver.save(sess,save_file)\n",
        "                \n",
        "        tbc.close()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK-SjQ4FSyBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}